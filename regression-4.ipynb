{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42bc6dac-bf92-44cb-bf95-6f5b0d8de04c",
   "metadata": {},
   "source": [
    "<span style=color:red;font-size:55px>ASSIGNMENT</span>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62419aca-a64b-4713-bcfd-b3765afc5699",
   "metadata": {},
   "source": [
    "<span style=color:pink;font-size:50px>REGRESSION-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a55cf7-d2fc-429d-942b-57c70d0cc8bf",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85832e9d-77c7-4c73-850b-7b0625067dbb",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47a85f-3fa1-4b76-9457-fa0636af475d",
   "metadata": {},
   "source": [
    "## Understanding Lasso Regression and Its Differences\n",
    "\n",
    "### What is Lasso Regression?\n",
    "\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator Regression, is a type of linear regression that incorporates regularization to improve the model's performance and address issues like overfitting. It adds a penalty term to the ordinary least squares (OLS) loss function, which penalizes the absolute values of the coefficients.\n",
    "\n",
    "### Key Features of Lasso Regression:\n",
    "\n",
    "1. **L1 Regularization:**\n",
    "   - Lasso Regression adds a penalty term to the OLS loss function, which is proportional to the sum of the absolute values of the coefficients (L1 norm). This penalty encourages sparsity in the coefficient estimates, effectively performing feature selection by setting some coefficients to zero.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - One of the significant advantages of Lasso Regression is its ability to perform feature selection by automatically shrinking the coefficients of less important features to zero. This makes Lasso Regression particularly useful when dealing with datasets containing a large number of features, where feature selection is essential to improve model interpretability and generalization.\n",
    "\n",
    "3. **Trade-off Between Bias and Variance:**\n",
    "   - Similar to Ridge Regression, Lasso Regression introduces a tuning parameter (lambda or alpha) that controls the strength of regularization. Increasing the value of lambda results in more aggressive shrinkage of coefficients towards zero, leading to higher bias but lower variance. Finding the optimal value of lambda requires techniques such as cross-validation.\n",
    "\n",
    "### Differences from Other Regression Techniques:\n",
    "\n",
    "1. **Ridge Regression:**\n",
    "   - Ridge Regression also incorporates regularization but uses the sum of squared coefficients (L2 norm) as the penalty term. Unlike Lasso Regression, Ridge Regression tends to shrink the coefficients towards zero gradually without necessarily setting them to zero. This makes Lasso Regression more suitable for feature selection.\n",
    "\n",
    "2. **Ordinary Least Squares (OLS) Regression:**\n",
    "   - OLS Regression is the standard linear regression technique that aims to minimize the sum of squared residuals. Unlike Lasso Regression, OLS does not include any regularization, which can lead to overfitting when dealing with high-dimensional datasets or datasets with multicollinearity.\n",
    "\n",
    "3. **Elastic Net Regression:**\n",
    "   - Elastic Net Regression combines L1 and L2 regularization penalties, offering a compromise between Ridge and Lasso Regression. It can handle situations where there are correlated predictors and many irrelevant features, providing both feature selection and coefficient shrinkage.\n",
    "\n",
    "In summary, Lasso Regression stands out from other regression techniques due to its ability to perform automatic feature selection by shrinking less important coefficients to zero. This feature makes it particularly useful for high-dimensional datasets and situations where model interpretability is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561d873-6535-49b1-a76d-a6f7128e8e97",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba47af3-fb0c-4d33-8e2a-c884e4f07643",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be461d-f404-479f-8f0c-5f0b81760130",
   "metadata": {},
   "source": [
    "## Main Advantage of Using Lasso Regression in Feature Selection\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select relevant features while setting less important features' coefficients to zero. This feature makes Lasso Regression particularly useful in scenarios with high-dimensional datasets or when dealing with datasets containing a large number of features.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Automatic Feature Selection:**\n",
    "   - Lasso Regression introduces an L1 regularization penalty that encourages sparsity in the coefficient estimates. As a result, Lasso Regression tends to shrink less important feature coefficients to zero, effectively performing automatic feature selection. This is particularly beneficial when dealing with datasets with many irrelevant or redundant features.\n",
    "\n",
    "2. **Improved Model Interpretability:**\n",
    "   - By automatically selecting relevant features, Lasso Regression helps improve the model's interpretability by focusing only on the most important predictors. This simplifies the model and makes it easier to understand and interpret, especially in fields where interpretability is crucial, such as healthcare or finance.\n",
    "\n",
    "3. **Prevention of Overfitting:**\n",
    "   - In addition to feature selection, Lasso Regression's regularization term helps prevent overfitting by penalizing the absolute values of the coefficients. This encourages simpler models with fewer features, reducing the risk of overfitting and improving the model's generalization performance on unseen data.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Lasso Regression can effectively handle multicollinearity, a situation where predictor variables are highly correlated with each other. By shrinking coefficients towards zero, Lasso Regression redistributes the importance among correlated features, effectively selecting one feature from each group of correlated features while setting the others to zero.\n",
    "\n",
    "### Example Use Case:\n",
    "\n",
    "Consider a dataset with hundreds or thousands of features, where many of these features may be irrelevant or redundant. By applying Lasso Regression, you can automatically identify the most relevant features while discarding the less important ones. This not only simplifies the model but also reduces the risk of overfitting and improves predictive performance on new data.\n",
    "\n",
    "In summary, the main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select relevant features while effectively reducing model complexity and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006f19d-5337-4f32-8920-0ef7e1d345f1",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca049ae2-74fa-445b-9720-cdef1a6838c9",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2558f8f-501c-4e39-b8e2-8f0511c7ac34",
   "metadata": {},
   "source": [
    "## Interpreting Coefficients of a Lasso Regression Model\n",
    "\n",
    "The coefficients of a Lasso Regression model represent the relationship between each independent variable and the target variable, accounting for the regularization introduced by the L1 penalty term. Interpreting these coefficients involves understanding their magnitude, sign, and implications for the target variable.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Magnitude of Coefficients:**\n",
    "   - The magnitude of the coefficients indicates the strength of the relationship between each independent variable and the target variable. Larger coefficients imply a stronger influence on the target variable, while smaller coefficients suggest a weaker influence.\n",
    "\n",
    "2. **Direction of Effect:**\n",
    "   - The sign of the coefficients (positive or negative) indicates the direction of the effect of the corresponding independent variable on the target variable. A positive coefficient suggests a positive relationship (increasing the independent variable increases the target variable), while a negative coefficient suggests a negative relationship (increasing the independent variable decreases the target variable).\n",
    "\n",
    "3. **Sparsity in Coefficients:**\n",
    "   - One of the key features of Lasso Regression is its ability to perform feature selection by setting less important feature coefficients to zero. Therefore, some coefficients may be exactly zero, indicating that the corresponding independent variables have been excluded from the model.\n",
    "\n",
    "4. **Comparison with Ordinary Least Squares (OLS) Regression:**\n",
    "   - In OLS regression, the coefficients represent the change in the target variable for a one-unit change in the corresponding independent variable, holding all other variables constant. In Lasso Regression, while the coefficients still represent the change in the target variable, they are adjusted to account for regularization and feature selection.\n",
    "\n",
    "### Example Interpretation:\n",
    "\n",
    "Suppose we have a Lasso Regression model with the following coefficients:\n",
    "- Coefficient for feature A: 0.5\n",
    "- Coefficient for feature B: -0.3\n",
    "\n",
    "Interpretation:\n",
    "- A one-unit increase in feature A is associated with a 0.5-unit increase in the target variable, holding other variables constant.\n",
    "- A one-unit increase in feature B is associated with a 0.3-unit decrease in the target variable, holding other variables constant.\n",
    "- Features with coefficients set to zero have been excluded from the model, indicating that they are not considered important predictors.\n",
    "\n",
    "Remember that interpretation should be done in the context of the specific dataset and domain knowledge. Additionally, caution should be exercised when multicollinearity is present, as Lasso Regression may arbitrarily select one feature from a group of highly correlated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862cbc7d-8322-455d-b06f-c8ed8dd5d841",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768e42c-db8b-4ecd-9ad6-69f7835d5997",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a3633-7cc0-41b1-a7c3-a5f8453f8166",
   "metadata": {},
   "source": [
    "## Tuning Parameters in Lasso Regression and Their Effects\n",
    "\n",
    "In Lasso Regression, there are primarily two tuning parameters that can be adjusted to control the model's performance and behavior: \n",
    "\n",
    "1. **Alpha (λ):**\n",
    "   - Alpha is the regularization parameter in Lasso Regression, controlling the strength of the penalty applied to the absolute values of the coefficients. It is analogous to the lambda parameter in Ridge Regression.\n",
    "   - Effect on Model's Performance:\n",
    "     - Increasing alpha increases the level of regularization, leading to more aggressive shrinkage of coefficients towards zero.\n",
    "     - Higher values of alpha result in sparser coefficient estimates, as more coefficients are pushed to exactly zero, effectively performing feature selection.\n",
    "     - Lower values of alpha decrease the level of regularization, allowing more coefficients to have non-zero values.\n",
    "   - Finding the optimal alpha value typically involves techniques such as cross-validation to balance model bias and variance.\n",
    "\n",
    "2. **Max Iterations:**\n",
    "   - Max Iterations specifies the maximum number of iterations allowed for the solver algorithm to converge to the optimal solution. The solver algorithm in Lasso Regression iteratively updates the coefficients until convergence.\n",
    "   - Effect on Model's Performance:\n",
    "     - If the maximum number of iterations is too low, the solver may not converge to the optimal solution, leading to suboptimal model performance.\n",
    "     - Increasing the maximum number of iterations can improve the likelihood of convergence, especially for complex datasets or when alpha is set to high values.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a scenario where you are building a Lasso Regression model to predict housing prices. By adjusting the alpha parameter, you can control the sparsity of the coefficient estimates. A higher alpha value may lead to a simpler model with fewer predictor variables, while a lower alpha value may result in a more complex model with more predictor variables.\n",
    "\n",
    "Similarly, adjusting the max iterations parameter ensures that the solver algorithm has sufficient iterations to converge to the optimal solution, thereby improving model performance and stability.\n",
    "\n",
    "In summary, tuning the alpha and max iterations parameters in Lasso Regression allows you to control the level of regularization and convergence behavior, ultimately affecting the model's performance and complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2d934-cd62-4762-bdcf-b41fa774e38b",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc3852-009b-4474-a2da-ffc6414dab33",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255bcdc7-e9f2-4da9-874f-4857d36774fc",
   "metadata": {},
   "source": [
    "## Using Lasso Regression for Non-linear Regression Problems\n",
    "\n",
    "Lasso Regression is primarily a linear regression technique that incorporates regularization to improve model performance and address issues like overfitting. However, with appropriate transformations and feature engineering, Lasso Regression can also be adapted for non-linear regression problems.\n",
    "\n",
    "### Techniques for Non-linear Regression with Lasso Regression:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Transforming the original features or creating new features using non-linear transformations can help capture non-linear relationships between the predictors and the target variable. For example, you can include polynomial features (e.g., squared terms, interaction terms) or apply transformations such as logarithmic or exponential transformations.\n",
    "\n",
    "2. **Kernel Methods:**\n",
    "   - Kernel methods, such as the kernel trick used in Support Vector Machines (SVMs), can be applied to transform the feature space into a higher-dimensional space where non-linear relationships can be captured linearly. This approach allows Lasso Regression to handle non-linearities implicitly.\n",
    "\n",
    "3. **Piecewise Linearization:**\n",
    "   - For complex non-linear relationships, you can divide the input space into smaller regions and fit separate linear models (Lasso Regression models) within each region. This piecewise linearization approach can capture the non-linearities effectively while still leveraging the benefits of Lasso Regression for regularization.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you have a dataset with a non-linear relationship between the predictor variable X and the target variable Y. To apply Lasso Regression to this non-linear regression problem:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Generate polynomial features of X (e.g., X^2, X^3) to capture the non-linear relationship.\n",
    "   - Include interaction terms between X and other variables to capture non-linear interactions.\n",
    "\n",
    "2. **Kernel Methods:**\n",
    "   - Apply a kernel function to transform the original feature space into a higher-dimensional space, where non-linear relationships can be captured linearly.\n",
    "   - Use techniques like the Radial Basis Function (RBF) kernel or polynomial kernel to map the data into a higher-dimensional space.\n",
    "\n",
    "3. **Piecewise Linearization:**\n",
    "   - Divide the range of X into smaller intervals and fit separate Lasso Regression models within each interval.\n",
    "   - Combine the predictions from all the models to obtain the final non-linear regression model.\n",
    "\n",
    "By applying these techniques, Lasso Regression can be adapted to handle non-linear regression problems effectively, providing regularization benefits while capturing complex non-linear relationships between the predictors and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40105e-cde9-40d5-9ed7-85190f286f29",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b50091-f485-4459-a52a-e9cb4b212989",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50866dea-cf84-4011-b840-7fc750b30acf",
   "metadata": {},
   "source": [
    "## Difference Between Ridge Regression and Lasso Regression\n",
    "\n",
    "Ridge Regression and Lasso Regression are two popular techniques used for linear regression with regularization. While both methods aim to prevent overfitting and improve model generalization, they differ in their approach to regularization and the type of penalty applied to the regression coefficients.\n",
    "\n",
    "### Ridge Regression:\n",
    "\n",
    "1. **Penalty Term:**\n",
    "   - Ridge Regression adds a penalty term to the ordinary least squares (OLS) loss function, which is proportional to the sum of the squared coefficients (L2 norm).\n",
    "   - Penalty Term: λ * ||β||₂²\n",
    "\n",
    "2. **Effect on Coefficients:**\n",
    "   - Ridge Regression shrinks the coefficients towards zero, but it does not necessarily set them exactly to zero. It reduces the magnitude of all coefficients, with smaller coefficients being shrunk more than larger ones.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - Ridge Regression does not perform feature selection in the same way as Lasso Regression. It tends to keep all features in the model but reduces their impact on the target variable.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Ridge Regression is effective in handling multicollinearity, where predictor variables are highly correlated. It redistributes the importance among correlated features by shrinking their coefficients proportionally.\n",
    "\n",
    "### Lasso Regression:\n",
    "\n",
    "1. **Penalty Term:**\n",
    "   - Lasso Regression also adds a penalty term to the OLS loss function, but it is proportional to the sum of the absolute values of the coefficients (L1 norm).\n",
    "   - Penalty Term: λ * ||β||₁\n",
    "\n",
    "2. **Effect on Coefficients:**\n",
    "   - Lasso Regression performs both shrinkage and variable selection. It tends to shrink the coefficients towards zero and can set some coefficients exactly to zero, effectively performing feature selection by excluding less important features from the model.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - Lasso Regression is known for its feature selection capabilities. It automatically selects a subset of relevant features while setting the coefficients of irrelevant features to zero.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - While Lasso Regression can handle multicollinearity to some extent, it may arbitrarily select one feature from a group of highly correlated features and set the coefficients of others to zero.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- Ridge Regression and Lasso Regression are both regularization techniques used to prevent overfitting in linear regression models.\n",
    "- Ridge Regression primarily shrinks the coefficients towards zero, whereas Lasso Regression performs shrinkage and feature selection by setting some coefficients exactly to zero.\n",
    "- Ridge Regression is preferred when multicollinearity is a concern and when retaining all features in the model is desirable. Lasso Regression is useful for feature selection and when a sparse model with fewer predictors is desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07626b89-275f-404e-860a-35a45b3b0470",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96700df2-12b6-46b4-b6eb-54ff5eaa21f7",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664177ab-fa0c-45da-94d9-629bc857d1db",
   "metadata": {},
   "source": [
    "## Handling Multicollinearity with Lasso Regression\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent, although its approach differs from Ridge Regression. Multicollinearity occurs when predictor variables in a regression model are highly correlated with each other, which can lead to unstable coefficient estimates. Lasso Regression addresses multicollinearity through its feature selection mechanism.\n",
    "\n",
    "### Mechanism of Lasso Regression in Handling Multicollinearity:\n",
    "\n",
    "1. **Variable Selection:**\n",
    "   - Lasso Regression automatically selects a subset of relevant features while setting the coefficients of less important features to zero. This feature selection mechanism effectively reduces the impact of multicollinearity by excluding redundant or less informative features from the model.\n",
    "\n",
    "2. **Shrinkage of Coefficients:**\n",
    "   - Lasso Regression shrinks the coefficients of highly correlated features towards zero. In the presence of multicollinearity, Lasso Regression may select one feature from a group of correlated features and set the coefficients of others to zero, effectively reducing the redundancy in the model.\n",
    "\n",
    "3. **Trade-off Between Features:**\n",
    "   - Lasso Regression's penalty term, which is proportional to the sum of the absolute values of the coefficients (L1 norm), encourages sparsity in the coefficient estimates. This encourages Lasso Regression to prefer solutions with fewer non-zero coefficients, leading to a simpler model that retains only the most relevant features.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Arbitrary Selection:**\n",
    "   - Lasso Regression may arbitrarily select one feature from a group of highly correlated features and set the coefficients of others to zero. The selected feature may vary depending on the dataset and the regularization strength (alpha parameter).\n",
    "\n",
    "2. **Impact on Interpretability:**\n",
    "   - While Lasso Regression's feature selection capability helps address multicollinearity, it may affect the interpretability of the model by excluding potentially relevant features. Careful consideration is required to ensure that important predictors are not inadvertently excluded from the model.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- Lasso Regression can handle multicollinearity by automatically selecting a subset of relevant features and setting the coefficients of less important features to zero.\n",
    "- Through its feature selection mechanism, Lasso Regression encourages sparsity in the coefficient estimates, leading to a simpler model with reduced redundancy.\n",
    "- However, Lasso Regression's arbitrary selection of features and its impact on model interpretability should be taken into account when applying it to datasets with multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9a06e-d9c2-4819-8d18-55044e6a4ced",
   "metadata": {},
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1055f-7e73-48ff-bd24-1ca8744f3e87",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563620d-9c5c-4dc8-9c69-56f1622279cd",
   "metadata": {},
   "source": [
    "## Choosing the Optimal Regularization Parameter (Lambda) in Lasso Regression\n",
    "\n",
    "Selecting the optimal value of the regularization parameter (lambda or alpha) in Lasso Regression is crucial for achieving the best model performance and balance between bias and variance. Various techniques can be employed to determine the optimal lambda value:\n",
    "\n",
    "### 1. Cross-Validation:\n",
    "\n",
    "Cross-validation is a common technique used to estimate the model's performance on unseen data and select the optimal regularization parameter. The process involves splitting the dataset into multiple subsets (folds), training the Lasso Regression model on different combinations of training and validation sets, and evaluating the model's performance on the validation sets.\n",
    "\n",
    "- **K-Fold Cross-Validation:** In K-fold cross-validation, the dataset is divided into K equal-sized folds. The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. The average performance across all folds is used to estimate the model's performance for each value of lambda.\n",
    "- **Leave-One-Out Cross-Validation (LOOCV):** LOOCV is a special case of K-fold cross-validation where each observation is used as a validation set once, and the model is trained on the remaining observations. This process is repeated for each observation, and the average performance is computed.\n",
    "\n",
    "### 2. Grid Search:\n",
    "\n",
    "Grid search involves specifying a range of lambda values and systematically evaluating the model's performance for each value within this range. The lambda value that yields the best performance metric (e.g., lowest mean squared error, highest R-squared) on a validation set is selected as the optimal lambda.\n",
    "\n",
    "- **Grid Search with Cross-Validation:** Grid search can be combined with cross-validation to perform a comprehensive search over the lambda values and estimate the model's performance more accurately.\n",
    "\n",
    "### 3. Regularization Path:\n",
    "\n",
    "Plotting the regularization path, which shows the coefficients of the Lasso Regression model as a function of lambda, can provide insights into the effect of regularization on the coefficients. The optimal lambda value can be chosen based on the point where the coefficients stabilize or start to approach zero.\n",
    "\n",
    "### 4. Information Criteria:\n",
    "\n",
    "Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used to select the optimal lambda value based on the trade-off between model fit and complexity. Lower values of AIC or BIC indicate better model fit with less complexity.\n",
    "\n",
    "### Example Code (Using Scikit-Learn):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d4d66a-5951-4887-8f6c-4191f8976e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha (lambda): 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Boston housing dataset from OpenML\n",
    "boston = fetch_openml(data_id=531)\n",
    "\n",
    "# Separate features and target variable\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create LassoCV model with cross-validation\n",
    "lasso_cv = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimal alpha (lambda)\n",
    "optimal_alpha = lasso_cv.alpha_\n",
    "print(\"Optimal alpha (lambda):\", optimal_alpha)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbb68c-8e0e-4fc0-b921-8dfe347225ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
